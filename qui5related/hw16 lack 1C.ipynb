{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 Classifying Extrema \n",
    "function to calculate df ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-x**2 + 3*x + 3, 3 - 2*x, 0.00131899999999983, 4.58200000000000)\n",
      "(-x**2 + 3*x + 3, 3 - 2*x, 0.00131900000000034, -4.58200000000000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "def getd(f_str, val):\n",
    "    x = sp.symbols('x')\n",
    "    formula = sp.sympify(f_str)\n",
    "    df = sp.diff(formula, x)\n",
    "    ddf = sp.diff(df, x)\n",
    "    d = df.evalf(subs={x:val})\n",
    "    dd = ddf.evalf(subs={x:val})\n",
    "    return df, ddf, d, dd\n",
    "f = \"-1*(x**3)/3 + 3*(x**2)/2 + 3*x + 12\"\n",
    "print(getd(f, -0.791))\n",
    "\n",
    "print(getd(f, 3.791))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.Determine the length of the interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7720626412559946\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "tau: 系数\n",
    "a0: left bound\n",
    "b0: right bound\n",
    "f: function\n",
    "'''\n",
    "import numpy as np\n",
    "a = 17.09\n",
    "b = 9.79\n",
    "c = 0.6317\n",
    "d = 0.9324\n",
    "e = 0.4565\n",
    "\n",
    "def f(x):\n",
    "    return a*x**4 + b*x**3 + c*x**2 + d*x + e\n",
    "\n",
    "def df(x):\n",
    "    return 4*a*x**3 + 3*b*x**2 + 2*c*x + d\n",
    "\n",
    "def d2f(x):\n",
    "    return 3*4*a*x**2 + 2*3*b*x + 2*c\n",
    "tau = (np.sqrt(5)-1)/2\n",
    "a0 = -7 #-2\n",
    "b0 = 12 #1\n",
    "\n",
    "\n",
    "h_k = b0 - a0  \n",
    "x1 = a0 + (1-tau) * h_k\n",
    "x2 = a0 + tau * h_k\n",
    "#print(x1,x2)\n",
    "f1 = f(x1)\n",
    "f2 = f(x2)\n",
    "\n",
    "errors = [np.abs(h_k)]\n",
    "count = 0\n",
    "\n",
    "while (count < 30 and np.abs(h_k) > 1e-6):\n",
    " \n",
    "    if  f1>f2:\n",
    "        a0 = x1\n",
    "        x1 = x2\n",
    "        f1 = f2\n",
    "        h_k = b0-a0\n",
    "        x2 = a0 + tau * h_k\n",
    "        f2 = f(x2)\n",
    "        count = count + 1\n",
    "        if count == 4:\n",
    "            print(h_k)\n",
    "    else:\n",
    "        b0 = x2\n",
    "        x2 = x1\n",
    "        f2 = f1\n",
    "        h_k = b0-a0\n",
    "        x1 = a0 + (1-tau) * h_k\n",
    "        f1 = f(x1)   \n",
    "        count = count + 1\n",
    "        if count == 4:\n",
    "            print(h_k)\n",
    "        \n",
    "    errors.append(np.abs(h_k))  \n",
    "    \n",
    "    #print(\"%10g \\t %10g \\t %12g %12g\" % (a0, b0, errors[-1], errors[-1]/errors[-2]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.2. Newton approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.0   292   230\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x0 = 2\n",
    "def f(x):\n",
    "    return 17*x**3 + 21*x**2 + 4*x + 2\n",
    "def df(x):\n",
    "    return 51*x**2 + 42*x + 4\n",
    "def df2(x):\n",
    "    return 102*x + 42\n",
    "c = f(x0)\n",
    "b = df(x0)\n",
    "a = 0.5*df2(x0)\n",
    "print(a,\" \",b,\" \",c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.4. Carrying out Newton steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.18823529411764717\n"
     ]
    }
   ],
   "source": [
    "#-e^(-x^2)\n",
    "import numpy as np\n",
    "def df(x):\n",
    "    return 2*x*np.exp(-x**2)\n",
    "def d2f(x):\n",
    "    return 2*np.exp(-x**2)*(1-2*x**2)\n",
    "x = 0.4 \n",
    "for i in range(1):\n",
    "    dfx = df(x)\n",
    "    d2fx = d2f(x)\n",
    "    xnew = x - dfx / d2fx\n",
    "    if np.abs(xnew-x) < 1e-8:\n",
    "        break\n",
    "    x = xnew \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.022793218738161758\n"
     ]
    }
   ],
   "source": [
    "#cosx \n",
    "import numpy as np\n",
    "def df(x):\n",
    "    return -1*np.sin(x)\n",
    "def d2f(x):\n",
    "    return -1*np.cos(x)\n",
    "x = 0.4 \n",
    "for i in range(1):\n",
    "    dfx = df(x)\n",
    "    d2fx = d2f(x)\n",
    "    xnew = x - dfx / d2fx\n",
    "    if np.abs(xnew-x) < 1e-8:\n",
    "        break\n",
    "    x = xnew \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.10. Newton on 2D Quadratic Function\n",
    "find iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.10004764, 0.00238209]), 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import sympy as sp\n",
    "def gradient(formula, symbols, values=None):\n",
    "    '''\n",
    "    Given a SymPy formula and variables\n",
    "    Find its analytic gradient without substituion\n",
    "    as a list of SymPy formulae or numerical gradient\n",
    "    if values specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    gradient = []\n",
    "    for i in range(size):\n",
    "        gradient.append(formula.diff(symbols[i]))\n",
    "        \n",
    "    if values == None:\n",
    "        return gradient\n",
    "\n",
    "    # Make sure you don't mess up the analytical gradient\n",
    "    g_copy = gradient.copy()\n",
    "    gradient_at = []\n",
    "    for i in range(len(g_copy)):\n",
    "        for j in range(len(symbols)):\n",
    "            g_copy[i] = g_copy[i].subs(symbols[j], values[j])\n",
    "        gradient_at.append(float(g_copy[i].evalf()))\n",
    "    return gradient_at\n",
    "def subs_all(formula, variables, values):\n",
    "    '''\n",
    "    You know what, it's getting to the point where this function\n",
    "    is necessary\n",
    "    '''\n",
    "    result = formula\n",
    "    for i in range(len(values)):\n",
    "        result = result.subs(variables[i], values[i])\n",
    "    return float(result.evalf())\n",
    "def hessian(gradient, symbols, values=None):\n",
    "    '''\n",
    "    Given an analytic gradient and variables\n",
    "    Calculate its analytic Hessian\n",
    "    or numerical Hessian if values are specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    hessian = []\n",
    "    for i in range(size):\n",
    "        hessian.append([0]*size)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian[i][j] = gradient[i].diff(symbols[j])\n",
    "            \n",
    "    if values == None:\n",
    "        return hessian\n",
    "    \n",
    "    hessian_at = hessian.copy()\n",
    "    size = len(hessian)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian_at[i][j] = subs_all(hessian_at[i][j], symbols, [float(v) for v in values])\n",
    "#             for k in range(len(symbols)):\n",
    "#                 hessian_at[i][j] = hessian_at[i][j].subs(symbols[k], values[k])\n",
    "#             hessian_at[i][j] = float(hessian_at[i][j].evalf())\n",
    "    return hessian_at\n",
    "def newton_nd_optimization_crude_step(formula, symbols, x_prev):\n",
    "    x_prev = list(x_prev)\n",
    "    grad = gradient(formula, symbols)\n",
    "    neg_gradient_at = -1 * np.array(gradient(formula, symbols, values=x_prev))\n",
    "    hes_at = np.array(hessian(grad, symbols, values=x_prev))\n",
    "    return np.array(x_prev) + la.solve(hes_at, neg_gradient_at)\n",
    "def newton_nd_optimization_crude(f_str, s_str, start, tolerance):\n",
    "    '''\n",
    "    A crude version of Newton ND Optimization algorithm\n",
    "    Maybe you can help out implementing an analytic solution\n",
    "    Returning the numerical solution as well as the number of\n",
    "    iterations it took\n",
    "    '''\n",
    "    formula = sp.sympify(f_str)\n",
    "    symbols = sp.symbols(s_str)\n",
    "    curr = np.copy(start)\n",
    "    iterations = 0\n",
    "    last = curr\n",
    "    maxi = 100\n",
    "    for i in range(maxi):\n",
    "        last = curr\n",
    "        \n",
    "        curr = newton_nd_optimization_crude_step(formula, symbols, curr)\n",
    "        if abs(la.norm(curr - last, 2))< tolerance:\n",
    "            break\n",
    "        iterations += 1\n",
    "    return curr, iterations\n",
    "# Workspace\n",
    "'''\n",
    "form = sp.sympify('16*x**2 - 6*x + 6 + 14*y**2 - x*y')\n",
    "symbols = sp.symbols('x y')\n",
    "values = [1, 2]\n",
    "hes = hessian(gradient(form, symbols), symbols, values)\n",
    "x_1 = newton_nd_optimization_crude_step(form, symbols, values)\n",
    "step = np.array(x_1) - np.array(values)\n",
    "print(hes)\n",
    "print(x_1)\n",
    "print(step)\n",
    "'''\n",
    "f_str = '25*x**2 - 5*x + 5 + 21*y**2 - x*y'\n",
    "s_str = 'x y'\n",
    "start = [1, 2]\n",
    "tolerance = 10**-9\n",
    "ans = newton_nd_optimization_crude(f_str, s_str, start, tolerance)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16.11 N-Dimension Optimization in Steps\n",
    "get accmulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad is  [73.0, 22.36508198777977]\n",
      "hes is  [[492.0, 73.0], [73.0, 10.173944288340007]]\n",
      "x1 is [-2.75169957 18.54570122]\n",
      "s0 is [-2.75169957 17.54570122]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import sympy as sp\n",
    "def gradient(formula, symbols, values=None):\n",
    "    '''\n",
    "    Given a SymPy formula and variables\n",
    "    Find its analytic gradient without substituion\n",
    "    as a list of SymPy formulae or numerical gradient\n",
    "    if values specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    gradient = []\n",
    "    for i in range(size):\n",
    "        gradient.append(formula.diff(symbols[i]))\n",
    "        \n",
    "    if values == None:\n",
    "        return gradient\n",
    "\n",
    "    # Make sure you don't mess up the analytical gradient\n",
    "    g_copy = gradient.copy()\n",
    "    gradient_at = []\n",
    "    for i in range(len(g_copy)):\n",
    "        for j in range(len(symbols)):\n",
    "            g_copy[i] = g_copy[i].subs(symbols[j], values[j])\n",
    "        gradient_at.append(float(g_copy[i].evalf()))\n",
    "    return gradient_at\n",
    "def subs_all(formula, variables, values):\n",
    "    '''\n",
    "    You know what, it's getting to the point where this function\n",
    "    is necessary\n",
    "    '''\n",
    "    result = formula\n",
    "    for i in range(len(values)):\n",
    "        result = result.subs(variables[i], values[i])\n",
    "    return float(result.evalf())\n",
    "def hessian(gradient, symbols, values=None):\n",
    "    '''\n",
    "    Given an analytic gradient and variables\n",
    "    Calculate its analytic Hessian\n",
    "    or numerical Hessian if values are specified\n",
    "    '''\n",
    "    size = len(symbols)\n",
    "    hessian = []\n",
    "    for i in range(size):\n",
    "        hessian.append([0]*size)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian[i][j] = gradient[i].diff(symbols[j])\n",
    "            \n",
    "    if values == None:\n",
    "        return hessian\n",
    "    \n",
    "    hessian_at = hessian.copy()\n",
    "    size = len(hessian)\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            hessian_at[i][j] = subs_all(hessian_at[i][j], symbols, [float(v) for v in values])\n",
    "#             for k in range(len(symbols)):\n",
    "#                 hessian_at[i][j] = hessian_at[i][j].subs(symbols[k], values[k])\n",
    "#             hessian_at[i][j] = float(hessian_at[i][j].evalf())\n",
    "    return hessian_at\n",
    "def newton_nd_optimization_crude_step(formula, symbols, x_prev):\n",
    "    x_prev = list(x_prev)\n",
    "    grad = gradient(formula, symbols)\n",
    "    neg_gradient_at = -1 * np.array(gradient(formula, symbols, values=x_prev))\n",
    "    hes_at = np.array(hessian(grad, symbols, values=x_prev))\n",
    "    return np.array(x_prev) + la.solve(hes_at, neg_gradient_at)\n",
    "\n",
    "form = sp.sympify('8*x**2 + 3*x*y + 8*y**2 + 10*E**(7*x*y) + 7*(sin(y)**2) + 14*cos(x*y)')\n",
    "symbols = sp.symbols('x y')\n",
    "values = [0, 1]\n",
    "grad = gradient(form, symbols, values)\n",
    "hes = hessian(gradient(form, symbols), symbols, values)\n",
    "x_1 = newton_nd_optimization_crude_step(form, symbols, values)\n",
    "step = np.array(x_1) - np.array(values)\n",
    "print(\"grad is \", grad)\n",
    "print(\"hes is \", hes)\n",
    "print(\"x1 is\", x_1)\n",
    "print(\"s0 is\", step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    a = x[0]\n",
    "    b = y[0]\n",
    "    return (x[0]-1)**2 + (x[1]-1)**2\n",
    "\n",
    "def df2(x):\n",
    "    return np.array([2*(x[0]-1),2*(x[1]-1) ])\n",
    "\n",
    "def ddf2(x):\n",
    "    return np.array([\n",
    "                     [2,0],\n",
    "                     [0,2]\n",
    "                     ])\n",
    "def steepestDescent(f,df,x0,maxiter,tol,alpha = 0):\n",
    "    # Line search function\n",
    "    def f_line(alpha):\n",
    "        fnew = f(x + alpha*s)\n",
    "        return fnew\n",
    "    steps = [x0]   \n",
    "    x = x0\n",
    "    fhist = [f(x)]\n",
    "    # Steepest descent with line search\n",
    "    for i in range(maxiter):\n",
    "        # Get the gradient\n",
    "        s = -df(x)\n",
    "        # Learning rate:\n",
    "        if alpha == 0:\n",
    "            # Perform line search\n",
    "            alpha_opt = sopt.golden(f_line)\n",
    "        else:\n",
    "            alpha_opt = alpha\n",
    "        # Steepest descent update\n",
    "        xnew = x + alpha_opt * s\n",
    "        # Save optimized solution for plotting\n",
    "        steps.append(xnew)\n",
    "        fhist.append(f(xnew))\n",
    "        # Check convergence\n",
    "        if ( np.abs(fhist[-1] - fhist[-2]) < tol ):\n",
    "            break\n",
    "        x = xnew\n",
    "    print('optimal solution is:', x)\n",
    "    return steps, fhist, i   \n",
    "\n",
    "x0 = np.array([-1.5, -1])\n",
    "# Steepest descent\n",
    "steps, fhist, iterations = steepestDescent(f2,df2,x0,50,1e-4)\n",
    "print('converged in', iterations, 'iterations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1 Golden Section Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.199994811011908\n",
      "3.2000026381129847\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def f(x):\n",
    "    return (x - 3.2)**2\n",
    "a = -1\n",
    "b = 8\n",
    "brackets = []\n",
    "gs = (np.sqrt(5) - 1) / 2\n",
    "m1 = a + (1 - gs) * (b - a)\n",
    "m2 = a + gs * (b - a)\n",
    "epsilon = 1e-5\n",
    "# Begin your modifications below here\n",
    "f1 = f(m1)\n",
    "f2 = f(m2)\n",
    "h = b-a\n",
    "while h >= epsilon:\n",
    "    if f1 > f2:\n",
    "        a = m1\n",
    "        m1 = m2\n",
    "        f1 = f2\n",
    "        h *= gs\n",
    "        m2 = a + gs * h\n",
    "        f2 = f(m2)\n",
    "        brackets.append([a, m1, m2, b])\n",
    "    else:\n",
    "        b = m2\n",
    "        m2 = m1\n",
    "        f2 = f1\n",
    "        h *= gs\n",
    "        m1 = a + (1 - gs) * h\n",
    "        f1 = f(m1)\n",
    "        brackets.append([a, m1, m2, b])\n",
    "    \n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C3 Using Newton's Method for 1D Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# complete the function below\n",
    "def dfunc(x):\n",
    "    # Add your code here\n",
    "    return np.exp(-1 * x**2)*(2*x**2 + 2*x*np.sin(x)-1-np.cos(x))\n",
    "    \n",
    "# complete the function below\n",
    "def d2func(x):\n",
    "    return np.exp(-x**2)*(-4*x**3 - 4*np.sin(x)*x**2 +6*x + 4*x *np.cos(x)+3*np.sin(x))\n",
    "    \n",
    "newton_guesses = [x0]\n",
    "x = x0\n",
    "for i in range(100):\n",
    "    if dfunc(x0) < tol:\n",
    "        break\n",
    "        \n",
    "    dfx = dfunc(x)\n",
    "    d2fx = d2func(x)\n",
    "    xnew = x - dfx / d2fx\n",
    "    newton_guesses.append(xnew)\n",
    "    if abs(dfunc(xnew)) <= tol:\n",
    "        break\n",
    "    x = xnew \n",
    "newton_guesses = np.array(newton_guesses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
